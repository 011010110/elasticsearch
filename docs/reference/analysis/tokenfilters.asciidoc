[[analysis-tokenfilters]]
== Token Filters

A _token filter_ receives the token stream from a
<<analysis-tokenizer,tokenizer>> (or from another token filter) and may add,
remove, or change tokens.  For example, a
<<analysis-lowercase-tokenfilter,`lowercase`>> token filter converts all
tokens to lowercase, a <<analysis-stop-tokenfilter,`stop`>> token filter
removes common words (_stop words_) like `the` from the token stream, and a
<<analysis-synonym-tokenfilter,`synonym`>> token filter introduces synonyms
into the token stream.

Token filters are not allowed to change the position or character offsets of
each token.

Elasticsearch has a number of built in token filters which can be used to
build <<analysis-custom-analyzer,custom analyzers>>.


[float]
=== Normalization Token Filters

The following token filters are used to apply some form of
language-independent normalization to all tokens:

<<analysis-apostrophe-tokenfilte,Apostrophe Token Filter>>::

    The `apostrophe` token filter removes apostrophes and any characters after
    the apostrophe from each token.

<<analysis-asciifolding-tokenfilter,ASCII Folding Token Filter>>::

    The `asciifolding` token filter converts alphabetic, numeric, and symbolic
    Unicode characters which are not in the first 127 ASCII characters (the
    "Basic Latin" Unicode block) into their ASCII equivalents, if one exists.

<<analysis-cjk-width-tokenfilter,CJK Width Token Filter>>::

    The `cjk_width` token filter normalized CJK width differences.

<<analysis-classic-tokenfilter,Classic Token Filter>>::

    The `classic` token filter post-processes tokens generated by the
    <<analysis-classic-tokenizer,`classic` tokenizer>>, removing the English
    possessive from the end of words, and removing dots from acronyms.

<<analysis-decimal-digit-tokenfilter,Decimal Digit Token Filter>>::

    The `decimal_digit` token filter folds unicode digits to `0-9`.

<<analysis-delimited-payload-tokenfilter,Delimited Payload Token Filter>>::

    The `delimited_payload` token filter accepts tokens with a particular
    format and splits the token into a token and its payload.

<<analysis-fingerprint-tokenfilter,Fingerprint Token Filter>>::

    The `fingerprint` token filter emits a single token which is useful for
    fingerprinting a body of text and for clustering.

<<analysis-limit-token-count-tokenfilter,Limit Token Count Token Filter>>::

    The `limit` token filter limits the number of tokens that are emitted.

<<analysis-lowercase-tokenfilter,Lowercase Token Filter>>::

    The `lowercase` token filter lowercases all tokens.

<<analysis-length-tokenfilter,Length Token Filter>>::

    The `length` token filter removes tokens that are longer or shorter than
    the specified limits.

<<analysis-pattern-replace-tokenfilter,Pattern Replace Token Filter>>::

    The `pattern_replace` token filter allows a regular expression to be used
    to replace matches with another string.

<<analysis-phonetic-tokenfilter,Phonetic Token Filter>>::

    The `phonetic` token filter passes each token through the specified
    phonetic encoder to produce a token representing the sound of the word.

<<analysis-reverse-tokenfilter,Reverse Token Filter>>::

  The `reverse` token filter reverses the order of the characters in each
  token.

<<analysis-standard-tokenfilter,Standard Token Filter>>::

    The `standard` token filter is used by the
    <<analysis-standard-analyzer,`standard` analyzer>> but currently does nothing.
    It serves as a placeholder for any future normalization which may be needed.

<<analysis-trim-tokenfilter,Trim Token Filter>>::

    The `trim` token filter removes leading and trailing whitespace.

<<analysis-truncate-tokenfilter,Truncate Token Filter>>::

    The `truncate` token filter truncates tokens which are longer then the
    specified maximum.

<<analysis-unique-tokenfilter,Unique Token Filter>>::

    The `unique` token filter removes duplicate tokens.

<<analysis-uppercase-tokenfilter,Uppercase Token Filter>>::

    The `uppercase` token filter uppercases all tokens.

[float]
=== Stopword and Synonym Token Filters

The following token filters allow the injection of synonyms into the stream,
or the removal of specified tokens from the stream:

<<analysis-synonym-tokenfilter,Synonym Token Filter>>::

    The `synonym` token filter injects synonyms into the token stream.

<<analysis-stop-tokenfilter,Stopword Token Filter>>::

    The `stop` token filter removes common words which have little
    impact on relevance from the token stream.

<<analysis-keep-words-tokenfilter,Keep Words Token Filter>>::

    The `keep_words` token filter will drop all tokens except for those in the
    specified list.

<<analysis-keep-types-tokenfilter,Keep Types Token Filter>>::

    The `keep_types` token filter will drop all tokens except those of the
    specified type.

[float]
=== Lanaguage and Stemming Token Filters

The following token filters provide human language oriented functionality like
decompounding compound words and stemming words to their root form:

<<analysis-stemmer-tokenfilter,Stemmer Token Filter>>::

    The `stemmer` token filter provides access to algorithmic stemmers for a
    wide range of languages.

<<analysis-hunspell-tokenfilter,Hunspell Token Filter>>::

    The `hunspell` token filter provides dictionary-based stemming.

<<analysis-compound-word-tokenfilter,Compoound Word Token Filter>>::

    The `hyphenation_decompounder` and `dictionary_decompounder` token filters
    can be used to break down compound words into their individual parts.

<<analysis-kstem-tokenfilter,KStem Token Filter>>::

    The `kstem` token filter is a high performance algorithmic stemmer for
    English, also accessible via the `stemmer` token filter.

<<analysis-porterstem-tokenfilter,Porter Stem Token Filter>>::

    The `porter_stem` token filter uses the Porter stemming algorithm for
    English, also accessible via the `stemmer` token filter.

<<analysis-snowball-tokenfilter,Snowball Token Filter>>::

    The `snowball` token filter provides Snowball-generated algorithmic
    stemming for a number of languages, also accessible via the `stemmer`
    token filter.

<<analysis-elision-tokenfilter,Elision Token Filter>>::

    The `elision` token filter removes elisions from words, converting
    `l'avion` into `avion`.

<<analysis-normalization-token-filter,Normalization Token Filter>>::

    The `normalization` token filter provides normalisers for a number of
    languages.

<<analysis-stemmer-override-tokenfilter,Stemmer Override Token Filter>>::

    The `stemmer_override` token filter allows you to override the default
    behaviour of a stemmer and to provide custom stemming rules for specific
    words.

<<analysis-keyword-marker-tokenfilter,Keyword Marker Token Filter>>::

    The `keyword_marker` token filter protects listed words from being
    stemmed.

<<analysis-keyword-repeat-tokenfilter,Keyword Repeat Token Filter>>::

    The `keyword_repeat` token filter, when used with a keyword-aware stemmer,
    emits each token twice: once in its original form and once in its stemmed
    form.


[float]
=== Partial Word Token Filters

The following token filters break up tokens into smaller fragments for partial
word matching, or combine adjacent tokens for matching of associated words:

<<analysis-ngram-tokenfilter,N-Gram Token Filter>>::

    The `ngram` token filter returns n-grams of each token: a sliding window
    of continuous letters, e.g. `quick` -> `[qu, ui, ic, ck]`.

<<analysis-edgengram-tokenfilter,Edge N-Gram Token Filter>>::

    The `edge_ngram` token filter returns n-grams of each token which are
    anchored to the start of the word, e.g. `quick` -> `[q, qu, qui, quic,
    quick]`.

<<analysis-shingle-tokenfilter,Shingle Token Filter>>::

    The `shingle` token filter creates N-grams of tokens by combining adjacent
    tokens. This is useful for finding associated words.

<<analysis-common-grams-tokenfilter,Common Grams Token Filter>>::

    The `common_grams` token filter emits every token as a unigram, but also
    creates bigrams of any common word (like `and` and `the`) and the
    subsequent word. This can greatly speed up phrase matching.

<<cjk-bigram-tokenfilter,CJK Bigram Token Filter>>::

    The `cjk_bigram` token filter forms bigrams out of adjacent CJK terms.

<<pattern-capture-tokenfilter,Pattern Capture Token Filter>>::

    The `pattern_capture` token filter allows multiple regular expressions to
    capture matching strings.

<<word-delimiter-tokenfilter,Word Delimiter Token Filter>>::

    The `word_delimiter` token filter splits words with embedded punctuation,
    numbers, or case change into sub-words, and optionally concatenates these
    sub-words without the punctuation.  Useful for product name matching.


include::tokenfilters/apostrophe-tokenfilter.asciidoc[]
include::tokenfilters/asciifolding-tokenfilter.asciidoc[]
include::tokenfilters/cjk-bigram-tokenfilter.asciidoc[]
include::tokenfilters/cjk-width-tokenfilter.asciidoc[]
include::tokenfilters/classic-tokenfilter.asciidoc[]
include::tokenfilters/common-grams-tokenfilter.asciidoc[]
include::tokenfilters/compound-word-tokenfilter.asciidoc[]
include::tokenfilters/decimal-digit-tokenfilter.asciidoc[]
include::tokenfilters/delimited-payload-tokenfilter.asciidoc[]
include::tokenfilters/edgengram-tokenfilter.asciidoc[]
include::tokenfilters/elision-tokenfilter.asciidoc[]
include::tokenfilters/fingerprint-tokenfilter.asciidoc[]
include::tokenfilters/hunspell-tokenfilter.asciidoc[]
include::tokenfilters/keep-types-tokenfilter.asciidoc[]
include::tokenfilters/keep-words-tokenfilter.asciidoc[]
include::tokenfilters/keyword-marker-tokenfilter.asciidoc[]
include::tokenfilters/keyword-repeat-tokenfilter.asciidoc[]
include::tokenfilters/kstem-tokenfilter.asciidoc[]
include::tokenfilters/length-tokenfilter.asciidoc[]
include::tokenfilters/limit-token-count-tokenfilter.asciidoc[]
include::tokenfilters/lowercase-tokenfilter.asciidoc[]
include::tokenfilters/ngram-tokenfilter.asciidoc[]
include::tokenfilters/normalization-tokenfilter.asciidoc[]
include::tokenfilters/pattern-capture-tokenfilter.asciidoc[]
include::tokenfilters/pattern_replace-tokenfilter.asciidoc[]
include::tokenfilters/phonetic-tokenfilter.asciidoc[]
include::tokenfilters/porterstem-tokenfilter.asciidoc[]
include::tokenfilters/reverse-tokenfilter.asciidoc[]
include::tokenfilters/shingle-tokenfilter.asciidoc[]
include::tokenfilters/snowball-tokenfilter.asciidoc[]
include::tokenfilters/standard-tokenfilter.asciidoc[]
include::tokenfilters/stemmer-override-tokenfilter.asciidoc[]
include::tokenfilters/stemmer-tokenfilter.asciidoc[]
include::tokenfilters/stop-tokenfilter.asciidoc[]
include::tokenfilters/synonym-tokenfilter.asciidoc[]
include::tokenfilters/trim-tokenfilter.asciidoc[]
include::tokenfilters/truncate-tokenfilter.asciidoc[]
include::tokenfilters/unique-tokenfilter.asciidoc[]
include::tokenfilters/uppercase-tokenfilter.asciidoc[]
include::tokenfilters/word-delimiter-tokenfilter.asciidoc[]




