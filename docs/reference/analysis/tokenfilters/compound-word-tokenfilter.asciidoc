[[analysis-compound-word-tokenfilter]]
=== Compound Word Token Filter

The `hyphenation_decompounder` and `dictionary_decompounder` token filters can
decompose compound words found in Germanic languages into their constituent
word parts, e.g. `Weißkopfseeadler` -> `[ Weiß, kopf, see, adler ]`.

Both token filters require a dictionary of subwords which will be recognised.

The `dictionary_decompounder` uses a brute force approach to find the subwords
from the dictionary in a compound word. It is much slower than the hyphenation
decompounder but can be used as a first start to check the quality of your
dictionary.

The `hyphenation_decompounder` uses hyphenation grammars to find potential
subwords that are then checked against the word dictionary. The quality of the
output tokens depends on the quality of the grammar file you use. For
languages like German they are quite good.

XML based hyphenation grammar files can be found in the
http://offo.sourceforge.net/hyphenation/#FOP+XML+Hyphenation+Patterns[Objects For Formatting Objects]
(OFFO) Sourceforge project. Currently only FOP v1.2 compatible hyphenation files
are supported. You can download https://sourceforge.net/projects/offo/files/offo-hyphenation/1.2/offo-hyphenation_v1.2.zip/download[offo-hyphenation_v1.2.zip]
directly and look in the `offo-hyphenation/hyph/` directory.
Credits for the hyphenation code go to the Apache FOP project .

[float]
=== Configuration

These token filters accept the following parameters:

[horizontal]
`type`::

    Either `dictionary_decompounder` or `hyphenation_decompounder`.

`word_list`::

    A array containing a list of words to use for the word dictionary.

`word_list_path`::

    The path to the word dictionary which may be absolute or relative to the
    `config` directory.

`hyphenation_patterns_path`::

    The path to a FOP XML hyphenation pattern file which may be absolute or
    relative to the `config` directory. (Only for `hyphenation_decompounder`).

`min_word_size`::

    Minimum word size. Defaults to `5`.

`min_subword_size`::

    Minimum subword size. Defaults to `2`.

`max_subword_size`::

    Maximum subword size. Defaults to `15`.

`only_longest_match`::

    Whether or not to include only the longest matching subword.  Defaults to
    `false`.


[float]
=== Example configuration

In this example, we configure the `dictionary_decompounder` token filter with
an inline list of words:

[source,js]
----------------------------
PUT my_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "standard",
          "filter": [ "lowercase", "my_filter" ]
        }
      },
      "filter": {
        "my_filter": {
          "type": "dictionary_decompounder",
          "word_list": ["weiß", "kopf" ]
        }
      }
    }
  }
}

GET _cluster/health?wait_for_status=yellow

POST my_index/_analyze
{
  "analyzer": "my_analyzer",
  "text": "Weißkopfseeadler"
}
----------------------------
// CONSOLE

/////////////////////

[source,js]
----------------------------
{
  "tokens": [
    {
      "token": "weißkopfseeadler",
      "start_offset": 0,
      "end_offset": 16,
      "type": "<ALPHANUM>",
      "position": 0
    },
    {
      "token": "weiß",
      "start_offset": 0,
      "end_offset": 16,
      "type": "<ALPHANUM>",
      "position": 0
    },
    {
      "token": "kopf",
      "start_offset": 0,
      "end_offset": 16,
      "type": "<ALPHANUM>",
      "position": 0
    }
  ]
}
----------------------------
// TESTRESPONSE

/////////////////////


The above example would emit the following terms:

[source,js]
----------------------------
[ weißkopfseeadler, weiß, kopf ]
----------------------------
